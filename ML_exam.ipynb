{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(url, names=['sepal.length', 'sepal.width', 'petal.length', 'petal.width', 'variety'],sep=',')\n",
    "#data = pd.read_excel(file)\n",
    "\n",
    "print('shape of the input data {}'.format(data.shape))\n",
    "random_state = 42 #use this to make the experiment repeatable\n",
    "\n",
    "#some methods for DataFrame \n",
    "data.columns #returns columns\n",
    "data.head(n=5)  #returns first n elements\n",
    "data.describe() #returns a short summary of descriptive statistics\n",
    "data['name_of_column'].describe() # descriptive statistics for specific column\n",
    "data.describe(include= \"O\") #also includes categorical attributes\n",
    "sorted(pd.unique(data['target_name'])) #unique takes as input an 1D array like\n",
    "\n",
    "pd.DataFrame.hist(data, figsize = [10,10])#show histograms. data can be a column\n",
    "plt.hist(data['sepal.width']) #histogram on single column\n",
    "plt.show()\n",
    "sns.pairplot(data, hue = 'variety', diag_kind='kde') #hue=attribute chosen as class\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Merge after changing from nominal to number\n",
    "ohe = OneHotEncoder(handle_unknown= \"ignore\", sparse= False)\n",
    "sex = ohe.fit_transform(df0[[\"Sex\"]])\n",
    "sex_df = pd.DataFrame(sex, columns= df0[\"Sex\"].unique())\n",
    "sex_df[\"Index\"] = sex_df.index\n",
    "\n",
    "df1 = df0.drop(\"Sex\", axis= 1).merge(sex_df, on= \"Index\").set_index(\"Index\")\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder(dtype = int)\n",
    "df[2] = enc.fit_transform(df[2].values.reshape(-1,1))\n",
    "\n",
    "######## Correlation ########################\n",
    "corr = data[data.columns].corr()\n",
    "plt.figure(figsize=(15,10)) # set X and Y size\n",
    "sns.heatmap(corr, cmap=\"YlGnBu\", annot=True)\n",
    "#############################################\n",
    "######## Boxplot ############################\n",
    "plt.figure(figsize=(15,15))\n",
    "pos = 1\n",
    "for i in data.columns[:-1]:\n",
    "    plt.subplot(2, 2, pos)\n",
    "    sns.boxplot(x= data['variety'], y = data[i], data = data) #with label and divided by class\n",
    "    pos += 1\n",
    "sns.boxplot(data = data) #alternative way (all in one image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "target_name = 'quality' # our y\n",
    "sorted(pd.unique(data[target_name])) #unique takes as input an 1D array like\n",
    "classes = sorted(data[target_name].unique())\n",
    "print(classes) \n",
    "#X = data.drop(labels='species',axis=1)\n",
    "#y = data.drop(labels=['sepal length', 'sepal width', 'petal length', 'petal width'],axis=1)\n",
    "X = data.drop(target_name,axis=1)\n",
    "y = data[target_name]\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,train_size=0.66,random_state=random_state)\n",
    "\n",
    "####### Decision Tree ##########################\n",
    "model = DecisionTreeClassifier(criterion='entropy',random_state=random_state)\n",
    "model.fit(Xtrain,ytrain)\n",
    "predicted_ytrain = model.predict(Xtrain)\n",
    "training_accuracy = accuracy_score(ytrain,predicted_ytrain)*100\n",
    "print('Accuracy on training set is {}%'.format(training_accuracy))\n",
    "predicted_ytest = model.predict(Xtest)\n",
    "testing_accuracy = accuracy_score(ytest,predicted_ytest)*100\n",
    "fitted_max_depth = model.tree_.max_depth # _ is used to access attributes in scikit_lear\n",
    "parameter_values = range(1,fitted_max_depth+1)\n",
    "print('Accuracy on training set is {}%'.format(testing_accuracy))\n",
    "print('Maximum depth of the tree is {}'.format(fitted_max_depth))\n",
    "\n",
    "###### Tuning with cross validation #############\n",
    "scores = []\n",
    "for par in parameter_values:\n",
    "    model = DecisionTreeClassifier(criterion='entropy', random_state=random_state, max_depth=par)\n",
    "    score = cross_val_score(model, Xtrain, ytrain, scoring = 'accuracy', cv = 5)\n",
    "    mean_score = np.mean(score)\n",
    "    scores.append(mean_score)\n",
    "print(scores)\n",
    "\n",
    "##### Printing the scores ####################\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(parameter_values,scores,'-o',linewidth=2, markersize=14)\n",
    "plt.xlabel('max depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title(\"Score with Cross Validation varying max_depth of tree\", fontsize = 15)\n",
    "plt.show\n",
    "\n",
    "###### Use the best hyperparameter #########\n",
    "best_parameter = parameter_values[np.argmax(scores)]\n",
    "model = DecisionTreeClassifier(criterion='entropy', random_state=random_state, max_depth=best_parameter)\n",
    "model.fit(Xtrain,ytrain)\n",
    "testing_accuracy_best_parameter = model.predict(Xtest)\n",
    "best_accuracy = accuracy_score(ytest,testing_accuracy_best_parameter)*100\n",
    "print('Accuracy on training set is {:.1f}% with depth {}'.format(best_accuracy,best_parameter))\n",
    "\n",
    "\n",
    "print(classification_report(ytest,testing_accuracy_best_parameter))\n",
    "\n",
    "print(confusion_matrix(ytest,testing_accuracy_best_parameter))\n",
    "\n",
    "print('\\t\\t\\t\\tAccuracy \\tHyperparameter')\n",
    "print('Simple Holdout and full tree: \\t{:0.1f}\\t\\t{}'.format(testing_accuracy,fitted_max_depth))\n",
    "print('CrossValidation and tuning: \\t{:0.1f}\\t\\t{}'.format(best_accuracy,best_parameter))\n",
    "\n",
    "####### plot Tree #######\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_tree(model,filled=True,feature_names=['sepal length', 'sepal width', 'petal length', 'petal width'],\\\n",
    "          class_names=['setosa', 'versicolor', 'virginica'],rounded=True,proportion=True)\n",
    "plt.show()\n",
    "\n",
    "maybe add last exercise: change quality to binary value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using several classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # uncomment this line to suppress warnings\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "print(__doc__) # print information included in the triple quotes at the beginning\n",
    "dataset = datasets.load_iris()\n",
    "ts = 0.3\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=ts,random_state=random_state)\n",
    "model_lbls = [\n",
    "              'dt', \n",
    "              'nb', \n",
    "              'lp', \n",
    "              'svc', \n",
    "             'knn',\n",
    "             'adb',\n",
    "             'rf',\n",
    "            ]\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_param_dt = [{'max_depth': [*range(1,20)]}]\n",
    "tuned_param_nb = [{'var_smoothing': [10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-07, 1e-8, 1e-9, 1e-10]}]\n",
    "tuned_param_lp = [{'early_stopping': [True]}]\n",
    "tuned_param_svc = [{'kernel': ['rbf'], \n",
    "                    'gamma': [1e-3, 1e-4],\n",
    "                    'C': [1, 10, 100, 1000],\n",
    "                    },\n",
    "                    {'kernel': ['linear'],\n",
    "                     'C': [1, 10, 100, 1000],                     \n",
    "                    },\n",
    "                   ]\n",
    "tuned_param_knn =[{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "tuned_param_adb = [{'n_estimators':[20,30,40,50],\n",
    "                   'learning_rate':[0.5,0.75,1,1.25,1.5]}]\n",
    "tuned_param_rf = [{'max_depth': [*range(5,15)],\n",
    "                   'n_estimators':[*range(10,100,10)]}]\n",
    "\n",
    "models = {\n",
    "    'dt': {'name': 'Decision Tree       ',\n",
    "           'estimator': DecisionTreeClassifier(), \n",
    "           'param': tuned_param_dt,\n",
    "          },\n",
    "    'nb': {'name': 'Gaussian Naive Bayes',\n",
    "           'estimator': GaussianNB(),\n",
    "           'param': tuned_param_nb\n",
    "          },\n",
    "    'lp': {'name': 'Linear Perceptron   ',\n",
    "           'estimator': Perceptron(),\n",
    "           'param': tuned_param_lp,\n",
    "          },\n",
    "    'svc':{'name': 'Support Vector      ',\n",
    "           'estimator': SVC(), \n",
    "           'param': tuned_param_svc\n",
    "          },\n",
    "    'knn':{'name': 'K Nearest Neighbor ',\n",
    "           'estimator': KNeighborsClassifier(),\n",
    "           'param': tuned_param_knn\n",
    "       },\n",
    "       'adb':{'name': 'AdaBoost           ',\n",
    "           'estimator': AdaBoostClassifier(),\n",
    "           'param': tuned_param_adb\n",
    "          },\n",
    "    'rf': {'name': 'Random forest       ',\n",
    "           'estimator': RandomForestClassifier(),\n",
    "           'param': tuned_param_rf\n",
    "          }\n",
    "}\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "def print_results(model):\n",
    "    print(\"Best parameters set found on train set:\")\n",
    "    print()\n",
    "    # if best is linear there is no gamma parameter\n",
    "    print(model.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on train set:\")\n",
    "    print()\n",
    "    means = model.cv_results_['mean_test_score']\n",
    "    stds = model.cv_results_['std_test_score']\n",
    "    params = model.cv_results_['params']\n",
    "    for mean, std, params_tuple in zip(means, stds, params):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params_tuple))\n",
    "    print()\n",
    "    print(\"Detailed classification report for the best parameter set:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full train set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "results_short = {}\n",
    "for score in scores:\n",
    "    print('='*40)\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    #'%s_macro' % score ## is a string formatting expression\n",
    "    # the parameter after % is substituted in the string placeholder %s\n",
    "    for m in model_lbls:\n",
    "        print('-'*40)\n",
    "        print(\"Trying model {}\".format(models[m]['name']))\n",
    "        \n",
    "        clf = GridSearchCV(models[m]['estimator'], models[m]['param'], cv=5,scoring='%s_macro' % score,n_jobs = 2) # this allows using multi-cores\n",
    "        clf.fit(X_train,y_train)\n",
    "        print_results(clf)\n",
    "        results_short[m] = clf.best_score_\n",
    "    print(\"Summary of results for {}\".format(score))\n",
    "    print(\"Estimator\")\n",
    "    for m in results_short.keys():\n",
    "        print(\"{}\\t - score: {:4.2}%\".format(models[m]['name'], results_short[m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "data.shape[0] - data.dropna().shape[0] # number of rows with null\n",
    "data = data.dropna() #axis 0 (row) by default\n",
    "######  Data Transformation #####################\n",
    "data['SexHRP']=data['SexHRP'].apply(lambda x: 0 if (x=='Female') else 1) #change value from Female,Male to 0,1\n",
    "#data.SexHRP.replace(('Male', 'Female'),(1,0),inplace=True)\n",
    "data['qmeat_hhsize_ratio'] = data['qmeat']/data['hhsize'] #new column created using 2 previous columns\n",
    "data['income_hhsize_ratio'] = data['income']/data['hhsize']\n",
    "data = data[['adults_n', 'children_n', 'SexHRP', 'AgeHRP', 'qmeat_hhsize_ratio', 'income_hhsize_ratio', 'uvmeat']]\n",
    "\n",
    "target = \"qmeat_hhsize_ratio\"\n",
    "y = data[target]\n",
    "X = data.drop(target, axis=1)\n",
    "\n",
    "####### two dimensional scatter plots \n",
    "##### for all the predicting variables with respect to the target\n",
    "ncols=3\n",
    "import math\n",
    "nrows = math.ceil((data.shape[1]-1)/ncols)\n",
    "figwidth = ncols * 7\n",
    "figheigth = nrows*5\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figwidth, figheigth),sharey=True)\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "fig.suptitle(\"Predicting variables versus target\", fontsize=18, y=0.95)\n",
    "\n",
    "for c, ax in zip(X.columns,axs.ravel()):\n",
    "    data.sort_values(by=c).plot.scatter(x=c,y=target, title = '\"{}\" versus \"{}\"'.format(target,c)\n",
    "                                    , ax=ax)\n",
    "plt.figure(figsize=(figwidth, figheigth))\n",
    "plt.suptitle(\"target versus predicting variables\")\n",
    "for i, col in enumerate(X.columns):\n",
    "    plt.subplot(nrows, ncols, i + 1)\n",
    "    plt.title(f\"\\\"{target}\\\" versus \\\"{data.columns[i-1]}\\\"\")\n",
    "    plt.scatter(X[col], y)\n",
    "    plt.ylabel(target)\n",
    "    plt.xlabel(col)\n",
    "\n",
    "###### Show p-values of targets with respect to variables\n",
    "from sklearn.feature_selection import f_regression\n",
    "# Your code here\n",
    "what is the p-value\n",
    "_, p_values = f_regression(X,y)\n",
    "p_values_show = pd.DataFrame({'Variable': X.columns, 'p-value': p_values})\n",
    "p_values_show\n",
    "\n",
    "###### Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=random_state, train_size=0.7)\n",
    "print('Training set and test set have {} and {} elements respectively'.format(X_train.shape[0],X_test.shape[0]))\n",
    "X_train.shape\n",
    "\n",
    "##### Consider a reduced dataset containing the chosen variable and the target\n",
    "pred_var = 'adults_n'\n",
    "X_train_r = X_train[pred_var].values.reshape(-1,1) # transform a series into a one-column array\n",
    "X_test_r = X_test[pred_var].values.reshape(-1,1)\n",
    "X_train_r.shape\n",
    "\n",
    "#####Fit the `linear_model` estimator on the training set and predict the target for the test set using the *fitted* estimator\n",
    "# Create linear regression object\n",
    "linear_uni = linear_model.LinearRegression()\n",
    "# Train the model using the training set\n",
    "linear_uni.fit(X_train_r, y_train)\n",
    "# Make predictions using the test set\n",
    "y_train_pred_uni = linear_uni.predict(X_train_r)\n",
    "y_test_pred_uni = linear_uni.predict(X_test_r)\n",
    "\n",
    "#### f-test #########\n",
    "def f_test(y_true, y_pred, n_var, n_obs, sn=.95):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    n = n_obs\n",
    "    p = n_var+1 # number of regression parameters (coefficients + intercept)\n",
    "    y_true_m = np.mean(y_true)\n",
    "    SSM = np.sum((y_pred-y_true_m)**2)\n",
    "    SST = np.sum((y_true-y_true_m)**2)\n",
    "    SSE = np.sum((y_true-y_pred)**2)\n",
    "    DFT = n - 1\n",
    "    DFM = p - 1 # degrees of freedom for model - numerator\n",
    "    DFE = n - p # degrees of freedom for error - denominator\n",
    "    DFT = n - 1\n",
    "    MSM = SSM / DFM\n",
    "    MSE = SSE / DFE \n",
    "    MST = SST / DFT\n",
    "    F = MSM / MSE\n",
    "    p = 1-scipy.stats.f.cdf(F, DFM, DFE) #find p-value of F test statistic \n",
    "    return F, p\n",
    "\n",
    "###### Compute the statistical significance of the model\n",
    "#perform F-test\n",
    "f_statistic_uni, p_value_uni = f_test(y_train, y_train_pred_uni, X_train_r.shape[1], X_train_r.shape[0])\n",
    "\n",
    "# The coefficient\n",
    "coeff_uni = linear_uni.coef_[0] # the coefficient is returned as a one-element list\n",
    "intercept_uni = linear_uni.intercept_\n",
    "# The root mean squared error\n",
    "rmse_uni = mean_squared_error(y_test, y_test_pred_uni, squared=False)\n",
    "# Coefficient of determination = 1 is perfect prediction\n",
    "r2_uni = r2_score(y_test, y_test_pred_uni)\n",
    "\n",
    "# The results are assembled in a dataframe for a compact view\n",
    "pd.DataFrame({'Univariate Linear - Value' : [intercept_uni\n",
    "                        , coeff_uni\n",
    "                        , rmse_uni\n",
    "                        , r2_uni\n",
    "                        , f_statistic_uni\n",
    "                        , p_value_uni]}\n",
    "            , index = ['Intercept for \"{}\"'.format(pred_var)\n",
    "                     , 'Coefficient for \"{}\"'.format(pred_var)\n",
    "                     , 'rmse'\n",
    "                     , 'r2'\n",
    "                     , 'f-statistic'\n",
    "                     , 'p-value'])\n",
    "\n",
    "####### Using the entire dataset ######################\n",
    "# Create linear regression object\n",
    "linear_multi = linear_model.LinearRegression()\n",
    "# Train the model using the training set\n",
    "linear_multi.fit(X_train, y_train)\n",
    "# Make predictions using the test set\n",
    "y_train_pred_multi = linear_multi.predict(X_train)\n",
    "y_test_pred_multi = linear_multi.predict(X_test)\n",
    "\n",
    "# Show the coefficients of the predicting variables\n",
    "pd.DataFrame({'Variable': X.columns, 'Coefficient': linear_multi.coef_})\n",
    "\n",
    "_, p_values = f_regression(X_train,y_train_pred_multi)\n",
    "p_values_show = pd.DataFrame({'Variable': X.columns, 'p-value': p_values})\n",
    "p_values_show\n",
    "\n",
    "#perform F-test\n",
    "f_statistic_multi, p_value_multi = f_test(y_train, y_train_pred_multi, X_train.shape[1], X_train.shape[0])\n",
    "                                        \n",
    "# The mean squared error\n",
    "rmse_multi = mean_squared_error(y_test, y_test_pred_multi, squared=False)\n",
    "# print(\"The MSE for the multivariate linear regression of '{}' is: {:8.2f}\"\\\n",
    "#     .format(target, rmse_dt))\n",
    "# Coefficient of determination=1 is perfect prediction\n",
    "r2_multi = r2_score(y_test, y_test_pred_multi)\n",
    "# print(\"The 'R square' for the multivariate linear regression of '{}' is: {:8.2f}\"\\\n",
    "#     .format(target, r2_dt))\n",
    "pd.DataFrame({'Multivariate Linear - Value' : [rmse_multi\n",
    "                        , r2_multi]}\n",
    "            , index = ['rmse'\n",
    "                     , 'r2'])\n",
    "\n",
    "# The results are assembled in a dataframe for a compact view\n",
    "pd.DataFrame({'Univariate Linear - Value' : [rmse_multi\n",
    "                        , r2_multi\n",
    "                        , f_statistic_multi\n",
    "                        , p_value_multi]}\n",
    "            , index = ['rmse'\n",
    "                     , 'r2'\n",
    "                     , 'f-statistic'\n",
    "                     , 'p-value']).style.format(precision=4)\n",
    "\n",
    "####### Decision Tree Multivariate Regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "data = DecisionTreeRegressor(random_state=random_state)\n",
    "# Train the model using the training set\n",
    "data.fit(X_train, y_train);\n",
    "max_max_depth = data.tree_.max_depth\n",
    "print(\"The maximum depth of the full Decision Tree Regressor is {}\".format(max_max_depth))\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'max_depth': list(range(1,max_max_depth))}\n",
    "# create the grid search cross validation object\n",
    "dt_gscv = GridSearchCV(estimator=DecisionTreeRegressor(random_state=random_state)\n",
    "                    , param_grid=param_grid\n",
    "                    , scoring='neg_mean_squared_error' # select model with minimum mse\n",
    "                    )\n",
    "dt_gscv.fit(X_train,y_train)\n",
    "dt_best = dt_gscv.best_estimator_ # the GridSearchCV returns the best estimator\n",
    "best_max_depth = dt_best.tree_.max_depth\n",
    "print(\"The optimal maximum depth for the decision tree is {}\".format(best_max_depth))\n",
    "\n",
    "# Make predictions using the test set\n",
    "y_test_pred_dt = dt_best.predict(X_test)\n",
    "\n",
    "rmse_dt = mean_squared_error(y_test, y_test_pred_dt, squared=False)\n",
    "\n",
    "print(\"Decision Tree Regression - RMSE = {:.2f}\".format(rmse_dt))\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize = (20,15))\n",
    "plot_tree(dt_best\n",
    "          , filled = True # fills nodes with colors related to classes darker color means higher purity\n",
    "          , feature_names = list(X.columns)\n",
    "          # , max_depth=2\n",
    "          , fontsize=18\n",
    "        #   , class_names = df[target].unique()\n",
    "         )\n",
    "\n",
    "\n",
    "\n",
    "###### Random Forest Multivariate Regression\n",
    "# Create Random Forest regression object\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=random_state)\n",
    "# for simplicity, we use as a maximum maximum depth of the tree the value found in\n",
    "# the unconstrained decision tree fitting\n",
    "param_grid_rf = {'max_depth': list(range(1,max_max_depth))\n",
    "}\n",
    "# create the grid search with cross validation\n",
    "rf_gscv = GridSearchCV(rf, param_grid=param_grid_rf, scoring='neg_mean_squared_error') # look for minimum mean square error\n",
    "# Train the model using the training set\n",
    "rf_gscv.fit(X_train, y_train)\n",
    "# the grid search returns the best estimator\n",
    "rf = rf_gscv.best_estimator_\n",
    "print(\"The optimal maximum depth for the trees in the random forest is {}\".format(rf.max_depth))\n",
    "\n",
    "y_test_pred_rf = rf.predict(X_test)\n",
    "rmse_rf = mean_squared_error(y_test, y_test_pred_rf, squared=False)\n",
    "print(\"Random Forest Regression - RMSE = {:.2f}\".format(rmse_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#### Skipped until data subdivision into X and y\n",
    "plt.scatter(X,y)\n",
    "plt.xlabel(\"Temperature\");\n",
    "plt.ylabel(\"Demand\");\n",
    "plt.show()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=random_state, train_size=0.7)\n",
    "print('Training set and test set have {} and {} elements respectively'.format(X_train.shape[0],X_test.shape[0]))\n",
    "\n",
    "def f_test(y_true, y_pred, n_var, n_obs, sn=.95):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    n = n_obs\n",
    "    p = n_var+1 # number of regression parameters (coefficients + intercept)\n",
    "    y_true_m = np.mean(y_true)\n",
    "    SSM = np.sum((y_pred-y_true_m)**2)\n",
    "    SST = np.sum((y_true-y_true_m)**2)\n",
    "    SSE = np.sum((y_true-y_pred)**2)\n",
    "    DFT = n - 1\n",
    "    DFM = p - 1 # degrees of freedom for model - numerator\n",
    "    DFE = n - p # degrees of freedom for error - denominator\n",
    "    DFT = n - 1\n",
    "    MSM = SSM / DFM\n",
    "    MSE = SSE / DFE \n",
    "    MST = SST / DFT\n",
    "    F = MSM / MSE\n",
    "    # f = np.var(x, ddof=1)/np.var(y, ddof=1) #calculate F test statistic \n",
    "    p = 1-scipy.stats.f.cdf(F, DFM, DFE) #find p-value of F test statistic \n",
    "    return F, p\n",
    "\n",
    "def print_eval(X, y, model):\n",
    "    pred = model.predict(X)\n",
    "    F, p = f_test(y, pred, X.shape[1], X.shape[0])\n",
    "    print(\" Mean squared error: \\t{:.5}\".format(mean_squared_error(y,pred)))\n",
    "    print(\" r2 score: \\t\\t{:.5}\".format(r2_score(y,pred)))\n",
    "    print(\" f-statistic: \\t\\t{:.5}\".format(F))\n",
    "    print(\" p-value: \\t\\t{:.5}\".format(p))\n",
    "    return mean_squared_error(pred, y), r2_score(pred, y), F, p\n",
    "\n",
    "####### First Experiment #########################\n",
    "lmodel = LinearRegression()\n",
    "lmodel.fit(X.temp.values.reshape(-1,1), y)\n",
    "lin = print_eval(X_test, y_test, lmodel)\n",
    "####### Visualize the prediction of the model #######\n",
    "lpred = lmodel.predict(np.arange(min(X.temp), max(X.temp)).reshape(-1,1))\n",
    "plt.plot(np.arange(min(X.temp), max(X.temp)),lpred, label = \"lin\", color = \"red\")\n",
    "plt.legend()\n",
    "plt.scatter(X,y)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "####### Second Experiment ##########\n",
    "polFea = PolynomialFeatures(2,include_bias=False)\n",
    "X_poly = polFea.fit_transform(X_train.values)#.reshape(-1,1))\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y_train)\n",
    "\n",
    "pol = print_eval(polFea.transform(X_test), y_test, model)\n",
    "\n",
    "pred = model.predict(polFea.transform((np.arange(min(X.temp), max(X.temp))).reshape(-1,1)))\n",
    "plt.plot(np.arange(min(X.temp), max(X.temp)),pred, label = \"poly\",color=\"red\")\n",
    "plt.legend()\n",
    "plt.scatter(X,y)\n",
    "plt.plot()\n",
    "\n",
    "\n",
    "####### Third Experiment ##########\n",
    "polFea = PolynomialFeatures(3,include_bias=False)\n",
    "print(\"Polynomial degree = 3\")\n",
    "X_poly = polFea.fit_transform(X_train.values)#.reshape(-1,1))\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y_train)\n",
    "\n",
    "pol3 = print_eval(polFea.transform(X_test), y_test, model)\n",
    "\n",
    "pred3 = model.predict(polFea.transform((np.arange(min(X.temp), max(X.temp))).reshape(-1,1)))\n",
    "plt.plot(np.arange(min(X.temp), max(X.temp)),pred3, label = \"poly\",color=\"red\")\n",
    "plt.legend()\n",
    "plt.scatter(X,y)\n",
    "plt.plot()\n",
    "\n",
    "####### Fourth Experiment ##########\n",
    "polFea = PolynomialFeatures(4,include_bias=False)\n",
    "print(\"Polynomial degree = 4\")\n",
    "X_poly = polFea.fit_transform(X_train.values)#.reshape(-1,1))\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y_train)\n",
    "\n",
    "pol4 = print_eval(polFea.transform(X_test), y_test, model)\n",
    "\n",
    "pred4 = model.predict(polFea.transform((np.arange(min(X.temp), max(X.temp))).reshape(-1,1)))\n",
    "plt.plot(np.arange(min(X.temp), max(X.temp)),pred4, label = \"poly\",color=\"red\")\n",
    "plt.legend()\n",
    "plt.scatter(X,y)\n",
    "plt.plot()\n",
    "\n",
    "performance = {\"linear \": [*lin],\n",
    "                \"polynomial d = 2\": [*pol],\n",
    "                \"polynomial d = 3\": [*pol3],\n",
    "                \"polynomial d = 4\": [*pol4] }\n",
    "res = pd.DataFrame(performance, index = ['rmse'\n",
    "                     , 'r2'\n",
    "                     , 'f-statistic'\n",
    "                     , 'p-value'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering (K-mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#dataset_copy = dataset.loc[\"Fresh\":\"Delicassen\"].apply(np.sqrt)\n",
    "X_sqrt = pd.concat([dataset.iloc[:,:2],dataset.iloc[:,2:].applymap(sqrt)],axis=1)\n",
    "mms = MinMaxScaler()\n",
    "X = pd.DataFrame(mms.fit_transform(X_sqrt), columns = X_sqrt.columns)\n",
    "\n",
    "####### Elbow method ##########\n",
    "k_range = list(range(2,11)) # set the range of k values to test \n",
    "parameters_km = [{'n_clusters': k_range}]\n",
    "pg = list(ParameterGrid(parameters_km))\n",
    "inertias_km = []\n",
    "silhouette_scores_km = []\n",
    "for i in range(len(pg)):\n",
    "    km = KMeans(**(pg[i]), random_state=random_state)\n",
    "    y_km = km.fit_predict(X)\n",
    "    inertias_km.append(km.inertia_)\n",
    "    silhouette_scores_km.append(silhouette_score(X,y_km))\n",
    "\n",
    "########## Plot the inertia and silhouette score\n",
    "def two_plots(x, y1, y2, xlabel, y1label, y2label):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_ylabel(y1label, color=color)\n",
    "    ax1.plot(x, y1, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel(y2label, color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(x, y2, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.set_ylim(0,1) # the axis for silhouette is [0,1]\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "\n",
    "    two_plots(x=k_range, y1=inertias_km, y2=silhouette_scores_km\n",
    "          , xlabel='Number of clusters', y1label='Inertias', y2label='Silhouette scores'\n",
    "         )\n",
    "    \n",
    "#### Choose max silhouette score and point of biggest change for inertias\n",
    "k=3\n",
    "km = KMeans(n_clusters=k, \n",
    "            random_state=random_state)\n",
    "y_km = km.fit_predict(X)\n",
    "print(\"Number of clusters = {}\\t- Distortion = {:6.2f}\\t- Silhouette score = {:4.2f}\"\\\n",
    "    .format(k,inertias_km[k_range.index(k)],silhouette_scores_km[k_range.index(k)]))\n",
    "\n",
    "### Pie graph ############\n",
    "clust_sizes_km = np.unique(y_km,return_counts=True)\n",
    "pd.DataFrame(clust_sizes_km[1]).plot.pie(y=0, autopct='%1.1f%%', );\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "parameters = [{'n_clusters': k_range\n",
    "                    , 'linkage' : ['ward', 'complete', 'average', 'single']}]\n",
    "pg = list(ParameterGrid(parameters))\n",
    "result_ac = []\n",
    "for i in range(len(pg)):\n",
    "    ac = AgglomerativeClustering(**(pg[i]))\n",
    "    y_ac = ac.fit_predict(X)\n",
    "    result_ac.append([pg[i]['linkage'],pg[i]['n_clusters'],silhouette_score(X,y_ac)])\n",
    "\n",
    "df_result_ac = pd.DataFrame(data = result_ac, columns=['linkage','n_clusters','silhouette_score'])\n",
    "df_result_ac.sort_values(by='silhouette_score', ascending=False).head(5)\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder()\n",
    "df_result_ac[\"linkage_enc\"] = oe.fit_transform(df_result_ac[\"linkage\"].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 15))\n",
    "ax = fig.add_subplot(projection= \"3d\")\n",
    "\n",
    "x, y, z = df_result_ac['linkage_enc'].values, df_result_ac['n_clusters'].values, df_result_ac['silhouette_score'].values\n",
    "\n",
    "bottom = np.zeros(df_result_ac.shape[0])\n",
    "width = .5 * np.ones(df_result_ac.shape[0])#np.ones_like(zpos)\n",
    "depth = .5 * np.ones(df_result_ac.shape[0])\n",
    "\n",
    "ax.bar3d(x, y, bottom, width, depth, z )\n",
    "ax.set_xlabel(\"linkage type\")\n",
    "ax.set_ylabel(\"n_clusters\")\n",
    "ax.set_zlabel(\"silhouette_score\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(df_result_ac.iloc[[1]])\n",
    "ac = AgglomerativeClustering(**(pg[1]))\n",
    "y_ac = ac.fit_predict(X)\n",
    "##### Pie graph\n",
    "clust_sizes_ac = np.unique(y_ac,return_counts=True)\n",
    "pd.DataFrame(clust_sizes_ac[1]).plot.pie(y=0, autopct='%1.1f%%', );\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dataset['cluster_km']=y_km\n",
    "sns.pairplot(data=dataset, hue='cluster_km')\n",
    "plt.show()\n",
    "\n",
    "dataset['cluster_ac']=y_ac\n",
    "sns.pairplot(data=dataset.drop('cluster_km',axis=1), hue='cluster_ac');\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import pair_confusion_matrix\n",
    "pcm = pair_confusion_matrix(y_km,y_ac)\n",
    "pcm / pcm.sum()\n",
    "print(\"The percentage of match between the two clustering schemes is {:6.2f}%\"\\\n",
    "    .format((pcm / pcm.sum()).diagonal().sum()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "### Inspect data#########\n",
    "sns.pairplot(pd.DataFrame(X))\n",
    "plt.show()\n",
    "\n",
    "X = X[:,[0,1]]\n",
    "focus = [0,1]\n",
    "\n",
    "plt.scatter(X[:,focus[0]], X[:,focus[1]]\n",
    "            , c='white'          # color filling the data markers\n",
    "            , edgecolors='black' # edge color for data markers\n",
    "            , marker='o'         # data marker shape, e.g. triangles (v<>^), square (s), star (*), ...\n",
    "            , s=50)              # data marker size\n",
    "plt.grid()  # plots a grid on the data\n",
    "plt.show()\n",
    "\n",
    "db = DBSCAN()\n",
    "y_db = db.fit_predict(X)\n",
    "db.eps,db.min_samples\n",
    "\n",
    "cluster_labels_all = np.unique(y_db)\n",
    "cluster_labels = cluster_labels_all[cluster_labels_all != -1]\n",
    "n_clusters = len(cluster_labels)\n",
    "if cluster_labels_all[0] == -1:\n",
    "    noise = True\n",
    "    print(\"There is noise\")\n",
    "else:\n",
    "    noise = False\n",
    "print(\"There is/are {} cluster(s)\".format(n_clusters-noise))\n",
    "\n",
    "######## Print Centroids with auxiliary function from other file\n",
    "cluster_centers = np.empty((n_clusters,X.shape[1]))\n",
    "for i in cluster_labels:\n",
    "    cluster_centers[i,:] = np.mean(X[y_db==i,:], axis = 0)\n",
    "plot_clusters(X,y_db,dim=(focus[0],focus[1]), points = cluster_centers)\n",
    "###################################################\n",
    "\n",
    "####### Find best hyperparams #############################\n",
    "param_grid = {'eps': list(np.arange(0.01, 1, 0.01)), 'min_samples': list(range(1,10,1))}\n",
    "params = list(ParameterGrid(param_grid))\n",
    "\n",
    "dbscan_out = pd.DataFrame(columns =  ['eps','min_samples','n_clusters','silhouette', 'unclust%'])\n",
    "for i in range(len(params)):\n",
    "    db = DBSCAN(**(params[i]))\n",
    "    y_db = db.fit_predict(X)\n",
    "    cluster_labels_all = np.unique(y_db)\n",
    "    cluster_labels = cluster_labels_all[cluster_labels_all != -1]\n",
    "    n_clusters = len(cluster_labels)\n",
    "    if n_clusters > 1:\n",
    "        X_cl = X[y_db!=-1,:]\n",
    "        y_db_cl = y_db[y_db!=-1]\n",
    "        silhouette = silhouette_score(X_cl,y_db_cl)\n",
    "        uncl_p = (1 - y_db_cl.shape[0]/y_db.shape[0]) * 100\n",
    "        dbscan_out.loc[len(dbscan_out)] = [db.eps, db.min_samples, n_clusters, silhouette, uncl_p]\n",
    "\n",
    "sil_thr = 0.7  # visualize results only for combinations with silhouette above the threshold\n",
    "unc_thr = 10 # visualize results only for combinations with unclustered% below the threshold\n",
    "n_clu_max_thr = 4\n",
    "dbscan_out[(dbscan_out['silhouette']>=sil_thr)\\\n",
    "         & (dbscan_out['unclust%']<=unc_thr)\\\n",
    "         & (dbscan_out['n_clusters']<=n_clu_max_thr)]\n",
    "\n",
    "\n",
    "############ Observe visually ##############\n",
    "db = DBSCAN(eps=0.99, min_samples=9)\n",
    "# db = DBSCAN(eps=0.05, min_samples=9)\n",
    "# db = DBSCAN(eps=0.16, min_samples=9)\n",
    "y_db = db.fit_predict(X)\n",
    "cluster_labels_all = np.unique(y_db)\n",
    "cluster_labels = cluster_labels_all[cluster_labels_all != -1]\n",
    "n_clusters = len(cluster_labels)\n",
    "\n",
    "cluster_centers = np.empty((n_clusters,X.shape[1]))\n",
    "for i in cluster_labels:\n",
    "    cluster_centers[i,:] = np.mean(X[y_db==i,:], axis = 0)\n",
    "\n",
    "print(\"There are {} clusters\".format(n_clusters))\n",
    "print(\"The cluster labels are {}\".format(cluster_labels))\n",
    "cluster_centers\n",
    "plot_clusters(X,y_db,dim=(focus[0],focus[1]), points = cluster_centers)\n",
    "\n",
    "\n",
    "silhouette = silhouette_samples(X,y_db)\n",
    "# from plot_silhouette import plot_silhouette  # python script provided separately\n",
    "from plot_silhouette_w_mean import plot_silhouette  # python script provided separately\n",
    "plot_silhouette(silhouette,y_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "def plot_clusters(X, y, dim, points,\n",
    "                  labels_prefix = 'cluster', \n",
    "                  points_name = 'centroids',\n",
    "                  colors = cm.tab10, # a qualitative map \n",
    "                      # https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "#                   colors = ['brown', 'orange', 'olive', \n",
    "#                             'green', 'cyan', 'blue', \n",
    "#                             'purple', 'pink'],\n",
    "#                   points_color = 'red'\n",
    "                  points_color = cm.tab10(10) # by default the last of the map (to be improved)\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    Plot a two dimensional projection of an array of labelled points\n",
    "    X:      array with at least two columns\n",
    "    y:      vector of labels, length as number of rows in X\n",
    "    dim:    the two columns to project, inside range of X columns, e.g. (0,1)\n",
    "    points: additional points to plot as 'stars'\n",
    "    labels_prefix: prefix to the labels for the legend ['cluster']\n",
    "    points_name:   legend name for the additional points ['centroids']\n",
    "    colors: a color map\n",
    "    points_color: the color for the points\n",
    "    \"\"\"\n",
    "    # plot the labelled (colored) dataset and the points\n",
    "    labels = np.unique(y)\n",
    "    for i in range(len(labels)):\n",
    "        color = colors(i / len(labels)) # choose a color from the map\n",
    "        plt.scatter(X[y==labels[i],dim[0]], \n",
    "                    X[y==labels[i],dim[1]], \n",
    "                    s=10, \n",
    "                    c = [color], # scatter requires a sequence of colors\n",
    "                    marker='s', \n",
    "                    label=labels_prefix+str(labels[i]))\n",
    "    plt.scatter(points[:,dim[0]], \n",
    "                points[:,dim[1]], \n",
    "                s=50, \n",
    "                marker='*', \n",
    "                c=[points_color], \n",
    "                label=points_name)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "def plot_silhouette(silhouette_vals, y, \n",
    " \t\t\t\t\tcolors = cm.tab10\n",
    "\t\t\t\t\t):\n",
    "    \"\"\"\n",
    "    Plotting silhouette scores for the individual samples of a labelled data set.\n",
    "    The scores will be grouped according to labels and sorted in descending order.\n",
    "    The bars are proportional to the score and the color is determined by the label.\n",
    "    \n",
    "    silhouette_vals: the silhouette values of the samples\n",
    "    y:               the labels of the samples\n",
    "    \n",
    "    \"\"\"\n",
    "    cluster_labels = np.unique(y)\n",
    "    n_clusters = len(cluster_labels)\n",
    "    y_ax_lower, y_ax_upper = 0, 0\n",
    "    yticks = []\n",
    "    for i, c in enumerate(cluster_labels): # generate pairs index, cluster_label\n",
    "        c_silhouette_vals = silhouette_vals[y==c] # extracts records with the current cluster label\n",
    "        c_silhouette_vals.sort() # sort the silhouette vals for the current class\n",
    "        y_ax_upper += len(c_silhouette_vals)\n",
    "        color = colors(i / n_clusters)\n",
    "        plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n",
    "                edgecolor='none', color=color)\n",
    "        yticks.append((y_ax_lower + y_ax_upper) / 2)\n",
    "        y_ax_lower += len(c_silhouette_vals)\n",
    "\n",
    "    silhouette_avg = np.mean(silhouette_vals)\n",
    "    plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n",
    "    plt.yticks(yticks, cluster_labels)# + 1)\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.xlabel('Silhouette coefficient')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('./figures/silhouette.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette(silhouette_vals, y, \n",
    " \t\t\t\t\tcolors = cm.tab10,\n",
    " \t\t\t\t\tplot_noise = False\n",
    "\t\t\t\t\t):\n",
    "    \"\"\"\n",
    "    Plotting silhouette scores for the individual samples of a labelled data set.\n",
    "    The scores will be grouped according to labels and sorted in descending order.\n",
    "    The bars are proportional to the score and the color is determined by the label.\n",
    "    \n",
    "    silhouette_vals: the silhouette values of the samples\n",
    "    y:               the labels of the samples\n",
    "    plot_noise:      boolean, assumes the noise to be labeled with a negative integer\n",
    "    \n",
    "    \"\"\"\n",
    "    cluster_labels = np.unique(y)\n",
    "    if not plot_noise:\n",
    "\t    cluster_labels = cluster_labels[cluster_labels != -1]\n",
    "    n_clusters = len(cluster_labels)\n",
    "    y_ax_lower, y_ax_upper = 0, 0\n",
    "    yticks = []\n",
    "    for i, c in enumerate(cluster_labels): # generate pairs index, cluster_label\n",
    "        c_silhouette_vals = silhouette_vals[y==c] # extracts records with the current cluster label\n",
    "        c_silhouette_vals.sort() # sort the silhouette vals for the current class\n",
    "        y_ax_upper += len(c_silhouette_vals)\n",
    "        color = colors(i / n_clusters)\n",
    "        plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n",
    "                edgecolor='none', color=color)\n",
    "        yticks.append((y_ax_lower + y_ax_upper) / 2)\n",
    "        c_silhouette_avg = np.mean(c_silhouette_vals)\n",
    "        plt.axvline(c_silhouette_avg\n",
    "         \t\t\t, ymin = y_ax_lower/len(silhouette_vals)\n",
    "         \t\t\t, ymax = y_ax_upper/len(silhouette_vals)\n",
    "        \t\t\t, color=color, linestyle=\"-.\"\n",
    "        \t\t\t) \n",
    "        y_ax_lower += len(c_silhouette_vals)\n",
    "\n",
    "\n",
    "    silhouette_avg = np.mean(silhouette_vals)\n",
    "    plt.axvline(silhouette_avg, color=\"black\", linestyle=\"--\") \n",
    "    plt.yticks(yticks, cluster_labels)\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.xlabel('Silhouette coefficient - Cluster means: -. Global mean: --')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('./figures/silhouette.png', dpi=300)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
